# ====================================================================================
# COMANDOS ÚTEIS PARA VAST.AI - FEATURE GENESIS
# ====================================================================================

# ====================================================================================
# 1. CONECTAR À INSTÂNCIA VAST.AI
# ====================================================================================

# Comando para conectar e manter terminal interativo:
ssh -p 29060 -o StrictHostKeyChecking=no -o ConnectTimeout=10 -o LogLevel=ERROR -i ~/.ssh/id_ed25519 -t root@ssh4.vast.ai

# ====================================================================================
# 2. COMANDOS PARA EXECUTAR NA INSTÂNCIA REMOTA
# ====================================================================================

# Matar processo orchestration/main.py
pkill -9 -f 'orchestration/main.py'

# Verificar se o processo foi morto (sem mostrar o próprio comando grep)
ps aux | grep orchestration/main.py | grep -v grep

# Alternativa mais limpa usando pgrep
pgrep -f orchestration/main.py

# Monitorar GPUs com auto-refresh (1 segundo) - VERSÃO MELHORADA
watch -n 1 'echo "=== INFORMAÇÕES DAS GPUs - $(date) ==="; echo ""; nvidia-smi --query-gpu=index,name,driver_version,memory.total,memory.used,memory.free,utilization.gpu,utilization.memory,temperature.gpu,power.draw,power.limit,clocks.current.graphics,clocks.current.memory,clocks.max.graphics,clocks.max.memory --format=csv,noheader,nounits; echo ""; echo "=== PROCESSOS USANDO GPU ==="; nvidia-smi pmon -c 1 -s um; echo ""; echo "=== INFORMAÇÕES DO SISTEMA ==="; echo "CPU: $(nproc) cores"; echo "RAM Total: $(free -h | grep Mem | awk "{print \$2}")"; echo "RAM Disponível: $(free -h | grep Mem | awk "{print \$7}")"; echo "RAM Usada: $(free -h | grep Mem | awk "{print \$3}")"; echo "Disco Disponível: $(df -h / | tail -1 | awk "{print \$4}")"; echo "Disco Usado: $(df -h / | tail -1 | awk "{print \$3}")"; echo "Load Average: $(uptime | awk -F"load average:" "{print \$2}")"'

# Versão mais compacta e organizada
watch -n 1 'clear; echo "=== MONITORAMENTO SISTEMA - $(date) ==="; echo ""; echo "🖥️  CPU: $(nproc) cores | Load: $(uptime | awk -F"load average:" "{print \$2}")"; echo "💾 RAM: $(free -h | grep Mem | awk "{print \$3}")/$(free -h | grep Mem | awk "{print \$2}") (usado/total) | Disponível: $(free -h | grep Mem | awk "{print \$7}")"; echo "💿 Disco: $(df -h / | tail -1 | awk "{print \$3}")/$(df -h / | tail -1 | awk "{print \$2}") (usado/total) | Disponível: $(df -h / | tail -1 | awk "{print \$4}")"; echo ""; echo "=== GPUs ==="; nvidia-smi --query-gpu=index,utilization.gpu,memory.used,memory.free --format=csv,noheader,nounits | awk -F, "{printf \"GPU %s: %s%% | Mem: %sMB usada | %sMB livre\\n\", \$1, \$2, \$3, \$4}"; echo ""; echo "=== PROCESSOS GPU ==="; nvidia-smi pmon -c 1 -s um'

# Versão SIMPLIFICADA - Apenas GPU essencial
watch -n 1 'clear; echo "=== GPUs - $(date) ==="; echo ""; nvidia-smi --query-gpu=index,utilization.gpu,memory.used,memory.free --format=csv,noheader,nounits | awk -F", " "{printf \"GPU %s: %s%% uso | %s MB usada | %s MB livre\\n\", \$1, \$2, \$3, \$4}"'

# Versão ULTRA SIMPLES - Só o que você pediu
watch -n 1 'clear; echo "=== GPUs - $(date) ==="; echo ""; nvidia-smi --query-gpu=index,utilization.gpu,memory.used,memory.free --format=csv,noheader,nounits | awk -F", " "{printf \"GPU %s: %s%% | %sMB usada | %sMB livre\\n\", \$1, \$2, \$3, \$4}"'

# ====================================================================================
# 3. COMANDOS ALTERNATIVOS PARA MONITORAMENTO
# ====================================================================================

# Monitoramento simples de GPU
nvidia-smi

# Monitoramento contínuo de GPU (atualiza a cada 2 segundos)
nvidia-smi -l 2

# Ver processos usando GPU
nvidia-smi pmon

# Informações detalhadas do sistema
htop

# Ver uso de memória
free -h

# Ver uso de disco
df -h

# Ver processos Python
ps aux | grep python

# ====================================================================================
# 4. COMANDOS PARA GERENCIAR O PIPELINE
# ====================================================================================

# Navegar para o diretório do projeto
cd /workspace/feature_genesis

# Ativar ambiente conda
conda activate dynamic-stage0
# ou
conda activate feature-genesis

# Executar o pipeline
python orchestration/main.py

# Executar em background
nohup python orchestration/main.py > pipeline.log 2>&1 &

# Ver logs em tempo real
tail -f pipeline.log

# ====================================================================================
# 5. COMANDOS PARA TÚNEIS SSH (se necessário)
# ====================================================================================

# Criar túnel SSH reverso para MySQL (do local para remoto)
ssh -R 3010:127.0.0.1:3010 -p PORTA root@HOST

# Verificar se túnel está funcionando
netstat -tlnp | grep 3010

# ====================================================================================
# 6. COMANDOS PARA SINCRONIZAÇÃO DE DADOS
# ====================================================================================

# Sincronizar dados do R2 (na instância remota)
rclone sync "R2:bucket_name" "/data" --progress

# Verificar arquivos sincronizados
ls -la /data/

# ====================================================================================
# 7. COMANDOS DE EMERGÊNCIA
# ====================================================================================

# Matar todos os processos Python
pkill -9 python

# Matar todos os processos do usuário
pkill -9 -u root

# Reiniciar serviços NVIDIA
sudo systemctl restart nvidia-persistenced

# Verificar status dos serviços
systemctl status nvidia-persistenced

# ====================================================================================
# 8. COMANDOS PARA DEBUGGING
# ====================================================================================

# Ver logs do sistema
journalctl -f

# Ver uso de recursos em tempo real
top

# Ver conexões de rede
netstat -tulpn

# ====================================================================================
# 9. COMANDOS PARA MONITORAMENTO DUAL (TMUX)
# ====================================================================================

# Executar pipeline com monitoramento dual (logs + dashboard)
./run_pipeline_vast.sh

# Monitorar logs do pipeline em tempo real
tail -f /tmp/vast_pipeline_*.log

# Ver logs de uma instância específica
tail -f /tmp/vast_pipeline_12345.log

# Ver últimas linhas dos logs
tail -n 50 /tmp/vast_pipeline_*.log

# Filtrar logs por tipo
grep -E "(ERROR|WARN|INFO)" /tmp/vast_pipeline_*.log

# Ver logs de erro apenas
grep -E "ERROR" /tmp/vast_pipeline_*.log

# Ver progresso do pipeline
grep -E "(PROGRESS|STAGE|FEATURES)" /tmp/vast_pipeline_*.log

# ====================================================================================
# 10. COMANDOS PARA MONITORAMENTO AVANÇADO
# ====================================================================================

# Monitorar logs em tempo real (filtrado)
tail -f /tmp/vast_pipeline_*.log | grep -E "(PROGRESS|STAGE|FEATURES|ALERTS)"

# Monitorar apenas erros e alertas
tail -f /tmp/vast_pipeline_*.log | grep -E "(ERROR|WARN|ALERT|CRITICAL)"

# Monitorar evolução de features
tail -f /tmp/vast_pipeline_*.log | grep -E "(features.*→|INPUT:|OUTPUT:)"

# Monitorar performance
tail -f /tmp/vast_pipeline_*.log | grep -E "(PERFORMANCE|GPU memory|RAM)"

# Dashboard simples com watch
watch -n 5 'echo "=== PIPELINE STATUS ==="; ps aux | grep orchestration/main.py | grep -v grep; echo ""; echo "=== ÚLTIMOS LOGS ==="; tail -5 /tmp/vast_pipeline_*.log 2>/dev/null || echo "Nenhum log encontrado"'

# ====================================================================================
# EXEMPLO DE FLUXO COMPLETO:
# ====================================================================================
# 1. vast ssh 25709061
# 2. pkill -9 -f 'orchestration/main.py'
# 3. ps aux | grep orchestration/main.py
# 4. ./run_pipeline_vast.sh  (executa pipeline e salva logs)
# 5. tail -f /tmp/vast_pipeline_*.log  (monitorar logs em outro terminal)
# 6. ssh -L 8888:localhost:8888 user@host  (acessar dashboard remotamente)
# 8. watch -n 1 'nvidia-smi --query-gpu=index,name,memory.used,utilization.gpu,temperature.gpu --format=csv,noheader,nounits'
# ====================================================================================
