# ====================================================================================
# COMANDOS √öTEIS PARA VAST.AI - FEATURE GENESIS
# ====================================================================================

# ====================================================================================
# 1. CONECTAR √Ä INST√ÇNCIA VAST.AI
# ====================================================================================

# Comando para conectar e manter terminal interativo:
ssh -p 29060 -o StrictHostKeyChecking=no -o ConnectTimeout=10 -o LogLevel=ERROR -i ~/.ssh/id_ed25519 -t root@ssh4.vast.ai

# ====================================================================================
# 2. COMANDOS PARA EXECUTAR NA INST√ÇNCIA REMOTA
# ====================================================================================

# Matar processo orchestration/main.py
pkill -9 -f 'orchestration/main.py'

# Verificar se o processo foi morto (sem mostrar o pr√≥prio comando grep)
ps aux | grep orchestration/main.py | grep -v grep

# Alternativa mais limpa usando pgrep
pgrep -f orchestration/main.py

# Monitorar GPUs com auto-refresh (1 segundo) - VERS√ÉO MELHORADA
watch -n 1 'echo "=== INFORMA√á√ïES DAS GPUs - $(date) ==="; echo ""; nvidia-smi --query-gpu=index,name,driver_version,memory.total,memory.used,memory.free,utilization.gpu,utilization.memory,temperature.gpu,power.draw,power.limit,clocks.current.graphics,clocks.current.memory,clocks.max.graphics,clocks.max.memory --format=csv,noheader,nounits; echo ""; echo "=== PROCESSOS USANDO GPU ==="; nvidia-smi pmon -c 1 -s um; echo ""; echo "=== INFORMA√á√ïES DO SISTEMA ==="; echo "CPU: $(nproc) cores"; echo "RAM Total: $(free -h | grep Mem | awk "{print \$2}")"; echo "RAM Dispon√≠vel: $(free -h | grep Mem | awk "{print \$7}")"; echo "RAM Usada: $(free -h | grep Mem | awk "{print \$3}")"; echo "Disco Dispon√≠vel: $(df -h / | tail -1 | awk "{print \$4}")"; echo "Disco Usado: $(df -h / | tail -1 | awk "{print \$3}")"; echo "Load Average: $(uptime | awk -F"load average:" "{print \$2}")"'

# Vers√£o mais compacta e organizada
watch -n 1 'clear; echo "=== MONITORAMENTO SISTEMA - $(date) ==="; echo ""; echo "üñ•Ô∏è  CPU: $(nproc) cores | Load: $(uptime | awk -F"load average:" "{print \$2}")"; echo "üíæ RAM: $(free -h | grep Mem | awk "{print \$3}")/$(free -h | grep Mem | awk "{print \$2}") (usado/total) | Dispon√≠vel: $(free -h | grep Mem | awk "{print \$7}")"; echo "üíø Disco: $(df -h / | tail -1 | awk "{print \$3}")/$(df -h / | tail -1 | awk "{print \$2}") (usado/total) | Dispon√≠vel: $(df -h / | tail -1 | awk "{print \$4}")"; echo ""; echo "=== GPUs ==="; nvidia-smi --query-gpu=index,utilization.gpu,memory.used,memory.free --format=csv,noheader,nounits | awk -F, "{printf \"GPU %s: %s%% | Mem: %sMB usada | %sMB livre\\n\", \$1, \$2, \$3, \$4}"; echo ""; echo "=== PROCESSOS GPU ==="; nvidia-smi pmon -c 1 -s um'

# Vers√£o SIMPLIFICADA - Apenas GPU essencial
watch -n 1 'clear; echo "=== GPUs - $(date) ==="; echo ""; nvidia-smi --query-gpu=index,utilization.gpu,memory.used,memory.free --format=csv,noheader,nounits | awk -F", " "{printf \"GPU %s: %s%% uso | %s MB usada | %s MB livre\\n\", \$1, \$2, \$3, \$4}"'

# Vers√£o ULTRA SIMPLES - S√≥ o que voc√™ pediu
watch -n 1 'clear; echo "=== GPUs - $(date) ==="; echo ""; nvidia-smi --query-gpu=index,utilization.gpu,memory.used,memory.free --format=csv,noheader,nounits | awk -F", " "{printf \"GPU %s: %s%% | %sMB usada | %sMB livre\\n\", \$1, \$2, \$3, \$4}"'

# ====================================================================================
# 3. COMANDOS ALTERNATIVOS PARA MONITORAMENTO
# ====================================================================================

# Monitoramento simples de GPU
nvidia-smi

# Monitoramento cont√≠nuo de GPU (atualiza a cada 2 segundos)
nvidia-smi -l 2

# Ver processos usando GPU
nvidia-smi pmon

# Informa√ß√µes detalhadas do sistema
htop

# Ver uso de mem√≥ria
free -h

# Ver uso de disco
df -h

# Ver processos Python
ps aux | grep python

# ====================================================================================
# 4. COMANDOS PARA GERENCIAR O PIPELINE
# ====================================================================================

# Navegar para o diret√≥rio do projeto
cd /workspace/feature_genesis

# Ativar ambiente conda
conda activate dynamic-stage0
# ou
conda activate feature-genesis

# Executar o pipeline
python orchestration/main.py

# Executar em background
nohup python orchestration/main.py > pipeline.log 2>&1 &

# Ver logs em tempo real
tail -f pipeline.log

# ====================================================================================
# 5. COMANDOS PARA T√öNEIS SSH (se necess√°rio)
# ====================================================================================

# Criar t√∫nel SSH reverso para MySQL (do local para remoto)
ssh -R 3010:127.0.0.1:3010 -p PORTA root@HOST

# Verificar se t√∫nel est√° funcionando
netstat -tlnp | grep 3010

# ====================================================================================
# 6. COMANDOS PARA SINCRONIZA√á√ÉO DE DADOS
# ====================================================================================

# Sincronizar dados do R2 (na inst√¢ncia remota)
rclone sync "R2:bucket_name" "/data" --progress

# Verificar arquivos sincronizados
ls -la /data/

# ====================================================================================
# 7. COMANDOS DE EMERG√äNCIA
# ====================================================================================

# Matar todos os processos Python
pkill -9 python

# Matar todos os processos do usu√°rio
pkill -9 -u root

# Reiniciar servi√ßos NVIDIA
sudo systemctl restart nvidia-persistenced

# Verificar status dos servi√ßos
systemctl status nvidia-persistenced

# ====================================================================================
# 8. COMANDOS PARA DEBUGGING
# ====================================================================================

# Ver logs do sistema
journalctl -f

# Ver logs do Docker (se usando)
docker logs CONTAINER_ID

# Ver uso de recursos em tempo real
top

# Ver conex√µes de rede
netstat -tulpn

# ====================================================================================
# EXEMPLO DE FLUXO COMPLETO:
# ====================================================================================
# 1. vast ssh 25709061
# 2. pkill -9 -f 'orchestration/main.py'
# 3. ps aux | grep orchestration/main.py
# 4. watch -n 1 'nvidia-smi --query-gpu=index,name,memory.used,utilization.gpu,temperature.gpu --format=csv,noheader,nounits'
# ====================================================================================
